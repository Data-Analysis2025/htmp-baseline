{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-30T17:17:07.108891Z",
     "iopub.status.busy": "2025-11-30T17:17:07.108526Z",
     "iopub.status.idle": "2025-11-30T17:17:12.014871Z",
     "shell.execute_reply": "2025-11-30T17:17:12.013735Z",
     "shell.execute_reply.started": "2025-11-30T17:17:07.108860Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import datetime\n",
    "import itertools\n",
    "from typing import List\n",
    "\n",
    "from tqdm import tqdm\n",
    "from dataclasses import dataclass, asdict\n",
    "\n",
    "import polars as pl \n",
    "import numpy as np\n",
    "from sklearn.linear_model import ElasticNet, ElasticNetCV, LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import kaggle_evaluation.default_inference_server\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Directory Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-11-30T17:17:12.017436Z",
     "iopub.status.busy": "2025-11-30T17:17:12.016762Z",
     "iopub.status.idle": "2025-11-30T17:17:12.030099Z",
     "shell.execute_reply": "2025-11-30T17:17:12.028850Z",
     "shell.execute_reply.started": "2025-11-30T17:17:12.017397Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-30T17:17:12.032006Z",
     "iopub.status.busy": "2025-11-30T17:17:12.031628Z",
     "iopub.status.idle": "2025-11-30T17:17:12.145744Z",
     "shell.execute_reply": "2025-11-30T17:17:12.144755Z",
     "shell.execute_reply.started": "2025-11-30T17:17:12.031973Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ============ PATHS ============\n",
    "DATA_PATH: Path = Path('/kaggle/input/hull-tactical-market-prediction/')\n",
    "\n",
    "# ============ RETURNS TO SIGNAL CONFIGS ============\n",
    "MIN_SIGNAL: float = 0.0                         # Minimum value for the daily signal \n",
    "MAX_SIGNAL: float = 2.0                         # Maximum value for the daily signal \n",
    "SIGNAL_MULTIPLIER: float = 400.0                # Multiplier of the OLS market forward excess returns predictions to signal \n",
    "\n",
    "# ============ MODEL CONFIGS ============\n",
    "CV: int = 10                                    # Number of cross validation folds in the model fitting\n",
    "L1_RATIO: float = 0.5                           # ElasticNet mixing parameter\n",
    "ALPHAS: np.ndarray = np.logspace(-4, 2, 100)    # Constant that multiplies the penalty terms\n",
    "MAX_ITER: int = 1000000                         # The maximum number of iterations\n",
    "\n",
    "# ============ FEATURE FLAGS ============\n",
    "USE_LIMITED_FEATURES: bool = True               # If True, restrict to the curated small set (incl. U1/U2)\n",
    "ENABLE_INTERACTIONS: bool = False               # If True, create pairwise crosses on the kept feature set\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataclasses Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-30T17:17:12.147194Z",
     "iopub.status.busy": "2025-11-30T17:17:12.146844Z",
     "iopub.status.idle": "2025-11-30T17:17:12.170133Z",
     "shell.execute_reply": "2025-11-30T17:17:12.169266Z",
     "shell.execute_reply.started": "2025-11-30T17:17:12.147159Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DatasetOutput:\n",
    "    X_train : pl.DataFrame \n",
    "    X_test: pl.DataFrame\n",
    "    y_train: pl.Series\n",
    "    y_test: pl.Series\n",
    "    scaler: StandardScaler\n",
    "\n",
    "@dataclass \n",
    "class ElasticNetParameters:\n",
    "    l1_ratio : float \n",
    "    cv: int\n",
    "    alphas: np.ndarray \n",
    "    max_iter: int \n",
    "    \n",
    "    def __post_init__(self): \n",
    "        if self.l1_ratio < 0 or self.l1_ratio > 1: \n",
    "            raise ValueError(\"Wrong initializing value for ElasticNet l1_ratio\")\n",
    "        \n",
    "@dataclass(frozen=True)\n",
    "class RetToSignalParameters:\n",
    "    signal_multiplier: float \n",
    "    min_signal : float = MIN_SIGNAL\n",
    "    max_signal : float = MAX_SIGNAL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set the Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-30T17:17:12.173298Z",
     "iopub.status.busy": "2025-11-30T17:17:12.172939Z",
     "iopub.status.idle": "2025-11-30T17:17:12.212064Z",
     "shell.execute_reply": "2025-11-30T17:17:12.211227Z",
     "shell.execute_reply.started": "2025-11-30T17:17:12.173273Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "ret_signal_params = RetToSignalParameters(\n",
    "    signal_multiplier= SIGNAL_MULTIPLIER\n",
    ")\n",
    "\n",
    "enet_params = ElasticNetParameters(\n",
    "    l1_ratio = L1_RATIO, \n",
    "    cv = CV, \n",
    "    alphas = ALPHAS, \n",
    "    max_iter = MAX_ITER\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Loading/Creating Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-30T17:17:12.213337Z",
     "iopub.status.busy": "2025-11-30T17:17:12.212986Z",
     "iopub.status.idle": "2025-11-30T17:17:12.237869Z",
     "shell.execute_reply": "2025-11-30T17:17:12.236945Z",
     "shell.execute_reply.started": "2025-11-30T17:17:12.213309Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def load_trainset() -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Loads and preprocesses the training dataset.\n",
    "\n",
    "    Returns:\n",
    "        pl.DataFrame: The preprocessed training DataFrame.\n",
    "    \"\"\"\n",
    "    return (\n",
    "        pl.read_csv(DATA_PATH / \"train.csv\")\n",
    "        .rename({'market_forward_excess_returns':'target'})\n",
    "        .with_columns(\n",
    "            pl.exclude('date_id').cast(pl.Float64, strict=False)\n",
    "        )\n",
    "        .head(-10)\n",
    "    )\n",
    "\n",
    "\n",
    "def load_testset() -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Loads and preprocesses the testing dataset.\n",
    "\n",
    "    Returns:\n",
    "        pl.DataFrame: The preprocessed testing DataFrame.\n",
    "    \"\"\"\n",
    "    return (\n",
    "        pl.read_csv(DATA_PATH / \"test.csv\")\n",
    "        .rename({'lagged_forward_returns':'target'})\n",
    "        .with_columns(\n",
    "            pl.exclude('date_id').cast(pl.Float64, strict=False)\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "def create_example_dataset(\n",
    "    df: pl.DataFrame,\n",
    "    use_limited_features: bool = USE_LIMITED_FEATURES,\n",
    "    enable_interactions: bool = ENABLE_INTERACTIONS,\n",
    ") -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Creates new features and cleans a DataFrame.\n",
    "\n",
    "    Args:\n",
    "        df (pl.DataFrame): The input Polars DataFrame.\n",
    "        use_limited_features (bool): Restrict to curated feature list (plus U1/U2) when True.\n",
    "        enable_interactions (bool): Add pairwise crosses on the kept feature set when True.\n",
    "\n",
    "    Returns:\n",
    "        pl.DataFrame: The DataFrame with new features, selected columns, and no null values.\n",
    "    \"\"\"\n",
    "    limited_vars: List[str] = [\n",
    "        \"S2\", \"E2\", \"E3\", \"P9\", \"S1\", \"S5\", \"I2\", \"P8\",\n",
    "        \"P10\", \"P12\", \"P13\", \"U1\", \"U2\",\n",
    "    ]\n",
    "\n",
    "    df = df.with_columns(\n",
    "        (pl.col(\"I2\") - pl.col(\"I1\")).alias(\"U1\"),\n",
    "        (pl.col(\"M11\") / ((pl.col(\"I2\") + pl.col(\"I9\") + pl.col(\"I7\")) / 3)).alias(\"U2\"),\n",
    "    )\n",
    "\n",
    "    base_feature_cols = [col for col in df.columns if col not in [\"date_id\", \"target\"]]\n",
    "    feature_cols = [col for col in limited_vars if col in df.columns] if use_limited_features else base_feature_cols\n",
    "\n",
    "    base = (\n",
    "        df.select([\"date_id\", \"target\"] + feature_cols)\n",
    "        .with_columns([\n",
    "            pl.col(col).fill_null(pl.col(col).ewm_mean(com=0.5))\n",
    "            for col in feature_cols\n",
    "        ])\n",
    "        .drop_nulls()\n",
    "    )\n",
    "\n",
    "    if not enable_interactions:\n",
    "        return base\n",
    "\n",
    "    interaction_exprs = [\n",
    "        (pl.col(a) * pl.col(b)).alias(f\"{a}_x_{b}\")\n",
    "        for a, b in itertools.combinations(feature_cols, 2)\n",
    "    ]\n",
    "\n",
    "    if interaction_exprs:\n",
    "        base = base.with_columns(interaction_exprs)\n",
    "\n",
    "    return base\n",
    "    \n",
    "\n",
    "def join_train_test_dataframes(train: pl.DataFrame, test: pl.DataFrame) -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Joins two dataframes by common columns and concatenates them vertically.\n",
    "\n",
    "    Args:\n",
    "        train (pl.DataFrame): The training DataFrame.\n",
    "        test (pl.DataFrame): The testing DataFrame.\n",
    "\n",
    "    Returns:\n",
    "        pl.DataFrame: A single DataFrame with vertically stacked data from common columns.\n",
    "    \"\"\"\n",
    "    common_columns: list[str] = [col for col in train.columns if col in test.columns]\n",
    "    \n",
    "    return pl.concat([train.select(common_columns), test.select(common_columns)], how=\"vertical\")\n",
    "\n",
    "\n",
    "def split_dataset(train: pl.DataFrame, test: pl.DataFrame, features: list[str]) -> DatasetOutput: \n",
    "    \"\"\"\n",
    "    Splits the data into features (X) and target (y), and scales the features.\n",
    "\n",
    "    Args:\n",
    "        train (pl.DataFrame): The processed training DataFrame.\n",
    "        test (pl.DataFrame): The processed testing DataFrame.\n",
    "        features (list[str]): List of features to used in model. \n",
    "\n",
    "    Returns:\n",
    "        DatasetOutput: A dataclass containing the scaled feature sets, target series, and the fitted scaler.\n",
    "    \"\"\"\n",
    "    X_train = train.drop(['date_id','target']) \n",
    "    y_train = train.get_column('target')\n",
    "    X_test = test.drop(['date_id','target']) \n",
    "    y_test = test.get_column('target')\n",
    "    \n",
    "    scaler = StandardScaler() \n",
    "    \n",
    "    X_train_scaled_np = scaler.fit_transform(X_train)\n",
    "    X_train = pl.from_numpy(X_train_scaled_np, schema=features)\n",
    "    \n",
    "    X_test_scaled_np = scaler.transform(X_test)\n",
    "    X_test = pl.from_numpy(X_test_scaled_np, schema=features)\n",
    "    \n",
    "    \n",
    "    return DatasetOutput(\n",
    "        X_train = X_train,\n",
    "        y_train = y_train, \n",
    "        X_test = X_test, \n",
    "        y_test = y_test,\n",
    "        scaler = scaler\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting Return Prediction to Signal\n",
    "\n",
    "Here is an example of a potential function used to convert a prediction based on the market forward excess return to a daily signal position. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-30T17:17:12.238920Z",
     "iopub.status.busy": "2025-11-30T17:17:12.238680Z",
     "iopub.status.idle": "2025-11-30T17:17:12.269523Z",
     "shell.execute_reply": "2025-11-30T17:17:12.268638Z",
     "shell.execute_reply.started": "2025-11-30T17:17:12.238903Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def convert_ret_to_signal(\n",
    "    ret_arr: np.ndarray,\n",
    "    params: RetToSignalParameters\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Converts raw model predictions (expected returns) into a trading signal.\n",
    "\n",
    "    Args:\n",
    "        ret_arr (np.ndarray): The array of predicted returns.\n",
    "        params (RetToSignalParameters): Parameters for scaling and clipping the signal.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: The resulting trading signal, clipped between min and max values.\n",
    "    \"\"\"\n",
    "    return np.clip(\n",
    "        ret_arr * params.signal_multiplier + 1, params.min_signal, params.max_signal\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looking at the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-30T17:17:12.270827Z",
     "iopub.status.busy": "2025-11-30T17:17:12.270576Z",
     "iopub.status.idle": "2025-11-30T17:17:12.803147Z",
     "shell.execute_reply": "2025-11-30T17:17:12.802049Z",
     "shell.execute_reply.started": "2025-11-30T17:17:12.270798Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train: pl.DataFrame = load_trainset()\n",
    "test: pl.DataFrame = load_testset() \n",
    "print(train.tail(3)) \n",
    "print(test.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating the Train and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-30T17:17:12.804751Z",
     "iopub.status.busy": "2025-11-30T17:17:12.804223Z",
     "iopub.status.idle": "2025-11-30T17:17:12.961115Z",
     "shell.execute_reply": "2025-11-30T17:17:12.960065Z",
     "shell.execute_reply.started": "2025-11-30T17:17:12.804688Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df: pl.DataFrame = join_train_test_dataframes(train, test)\n",
    "df = create_example_dataset(\n",
    "    df=df,\n",
    "    use_limited_features=USE_LIMITED_FEATURES,\n",
    "    enable_interactions=ENABLE_INTERACTIONS,\n",
    ")\n",
    "train: pl.DataFrame = df.filter(pl.col('date_id').is_in(train.get_column('date_id')))\n",
    "test: pl.DataFrame = df.filter(pl.col('date_id').is_in(test.get_column('date_id')))\n",
    "\n",
    "FEATURES: list[str] = [col for col in test.columns if col not in ['date_id', 'target']]\n",
    "\n",
    "dataset: DatasetOutput = split_dataset(train=train, test=test, features=FEATURES) \n",
    "\n",
    "X_train: pl.DataFrame = dataset.X_train\n",
    "X_test: pl.DataFrame = dataset.X_test\n",
    "y_train: pl.Series = dataset.y_train\n",
    "y_test: pl.Series = dataset.y_test\n",
    "\n",
    "scaler: StandardScaler = dataset.scaler\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting the Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-30T17:17:12.962589Z",
     "iopub.status.busy": "2025-11-30T17:17:12.962248Z",
     "iopub.status.idle": "2025-11-30T17:17:13.226877Z",
     "shell.execute_reply": "2025-11-30T17:17:13.226107Z",
     "shell.execute_reply.started": "2025-11-30T17:17:12.962560Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model_cv: ElasticNetCV = ElasticNetCV(\n",
    "    **asdict(enet_params)\n",
    ")\n",
    "model_cv.fit(X_train, y_train) \n",
    "        \n",
    "# Fit the final model using the best alpha found by cross-validation\n",
    "model: ElasticNet = ElasticNet(alpha=model_cv.alpha_, l1_ratio=enet_params.l1_ratio) \n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction Function via Kaggle Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-30T17:17:13.227854Z",
     "iopub.status.busy": "2025-11-30T17:17:13.227611Z",
     "iopub.status.idle": "2025-11-30T17:17:13.233438Z",
     "shell.execute_reply": "2025-11-30T17:17:13.232774Z",
     "shell.execute_reply.started": "2025-11-30T17:17:13.227833Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def predict(test: pl.DataFrame) -> float:\n",
    "    test = test.rename({'lagged_forward_returns':'target'})\n",
    "    df: pl.DataFrame = create_example_dataset(\n",
    "        df=test,\n",
    "        use_limited_features=USE_LIMITED_FEATURES,\n",
    "        enable_interactions=ENABLE_INTERACTIONS,\n",
    "    )\n",
    "    X_test: pl.DataFrame = df.select(FEATURES)\n",
    "    X_test_scaled_np: np.ndarray = scaler.transform(X_test)\n",
    "    X_test: pl.DataFrame = pl.from_numpy(X_test_scaled_np, schema=FEATURES)\n",
    "    raw_pred: float = model.predict(X_test)[0]\n",
    "    return convert_ret_to_signal(raw_pred, ret_signal_params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Launch Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-30T17:17:13.234440Z",
     "iopub.status.busy": "2025-11-30T17:17:13.234180Z",
     "iopub.status.idle": "2025-11-30T17:17:13.596047Z",
     "shell.execute_reply": "2025-11-30T17:17:13.595210Z",
     "shell.execute_reply.started": "2025-11-30T17:17:13.234422Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "inference_server = kaggle_evaluation.default_inference_server.DefaultInferenceServer(predict)\n",
    "\n",
    "if os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n",
    "    inference_server.serve()\n",
    "else:\n",
    "    inference_server.run_local_gateway(('/kaggle/input/hull-tactical-market-prediction/',))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 14348714,
     "sourceId": 111543,
     "sourceType": "competition"
    },
    {
     "datasetId": 8298722,
     "sourceId": 13176071,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31089,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}